{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Explore Valid User IDs and Get Recommendations\n",
        "\n",
        "This notebook lists valid `user_id` values from the processed dataset and queries the running FastAPI service for recommendations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Using incubator modules: jdk.incubator.vector\n",
            "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
            "25/11/19 15:19:47 WARN Utils: Your hostname, andres-ThinkPad-L15-Gen-2, resolves to a loopback address: 127.0.1.1; using 192.168.1.11 instead (on interface wlp9s0)\n",
            "25/11/19 15:19:47 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
            "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
            "25/11/19 15:19:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample user_ids:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A2WLOSU1RW6TWC\n",
            "A2WLQEY91NZMU0\n",
            "A2WLRI5ONTW5T7\n",
            "A2WLVS5XJU8WPZ\n",
            "A2WM1G7B5YU0CT\n",
            "A2WM3V2HY2A5LU\n",
            "A2WM5CEGQTPDFK\n",
            "A2WMEWFRJOD82Z\n",
            "A2WMFRF7EP14U7\n",
            "A2WMLZMLETBCLD\n",
            "A2WMVHB96UXEEU\n",
            "A2WN0GMMFS8M8F\n",
            "A2WNJB2OIGP9RB\n",
            "A2WNJNOH8YQ6D1\n",
            "A2WNONSBFPS2S\n",
            "A2WNXXVE3PJDX4\n",
            "A2WO2E77X7YFAS\n",
            "A2WO5D8W8ZXPC9\n",
            "A2WO5O9H80Z994\n",
            "A2WOCCS2BOZ2AG\n",
            "\n",
            "Total users: 192,403\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Start Spark and load user map\n",
        "spark = SparkSession.builder.appName(\"user-explore\").getOrCreate()\n",
        "user_map_path = Path(\"../data/processed/user_map.parquet\")\n",
        "user_map = spark.read.parquet(str(user_map_path))\n",
        "\n",
        "print(\"Sample user_ids:\")\n",
        "user_ids = [r[0] for r in user_map.select(\"user_id\").limit(20).collect()]\n",
        "for uid in user_ids:\n",
        "    print(uid)\n",
        "\n",
        "print(f\"\\nTotal users: {user_map.count():,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using: http://localhost:8000 USER_ID= A2WLOSU1RW6TWC\n"
          ]
        }
      ],
      "source": [
        "# Configure API base URL and choose a user_id from the list printed above\n",
        "API_BASE = \"http://localhost:8000\"\n",
        "USER_ID = user_ids[0] if user_ids else None  # you can override this manually\n",
        "N = 5\n",
        "print(\"Using:\", API_BASE, \"USER_ID=\", USER_ID)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Status: 200\n",
            "{\n",
            "  \"user_id\": \"A2WLOSU1RW6TWC\",\n",
            "  \"items\": [\n",
            "    {\n",
            "      \"item_id\": \"B000N64DB8\",\n",
            "      \"score\": 5.29327917098999\n",
            "    },\n",
            "    {\n",
            "      \"item_id\": \"B005RF2VP0\",\n",
            "      \"score\": 5.2782816886901855\n",
            "    },\n",
            "    {\n",
            "      \"item_id\": \"B001TXBBQY\",\n",
            "      \"score\": 5.14666748046875\n",
            "    },\n",
            "    {\n",
            "      \"item_id\": \"B000VWG60A\",\n",
            "      \"score\": 5.137851238250732\n",
            "    },\n",
            "    {\n",
            "      \"item_id\": \"B00AGBPI3M\",\n",
            "      \"score\": 5.104048728942871\n",
            "    }\n",
            "  ]\n",
            "}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/11/19 18:32:55 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 1754697 ms exceeds timeout 120000 ms\n",
            "25/11/19 18:32:55 WARN SparkContext: Killing executors is not supported by current scheduler.\n",
            "25/11/19 18:32:57 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.11:35627\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/11/19 18:32:57 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.11:35627\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/11/19 18:33:07 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.11:35627\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/11/19 18:33:07 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.11:35627\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/11/19 18:33:17 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.11:35627\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/11/19 18:33:17 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.11:35627\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/11/19 18:33:27 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.11:35627\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/11/19 18:33:27 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.11:35627\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/11/19 18:33:37 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.11:35627\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/11/19 18:33:37 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.11:35627\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/11/19 18:33:47 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.11:35627\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/11/19 18:33:47 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.11:35627\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/11/19 19:17:48 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.11:35627\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/11/19 19:17:48 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.11:35627\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/11/19 19:17:58 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.11:35627\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/11/19 19:17:58 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.11:35627\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/11/19 19:18:08 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.11:35627\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/11/19 19:18:08 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.11:35627\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/11/19 19:18:18 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.11:35627\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/11/19 19:18:18 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.11:35627\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/11/19 19:18:28 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.11:35627\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/11/19 19:18:28 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.11:35627\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/11/19 19:18:38 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.11:35627\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/11/19 19:18:38 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.11:35627\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/11/19 19:18:48 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.11:35627\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/11/19 19:18:48 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.11:35627\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/11/19 19:18:58 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.11:35627\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/11/19 19:18:58 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.11:35627\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/11/19 19:19:08 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.11:35627\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/11/19 19:19:08 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.11:35627\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/11/19 19:19:18 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.11:35627\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/11/19 19:19:18 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.11:35627\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/11/19 19:19:28 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.11:35627\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/11/19 19:19:28 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.11:35627\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "if not USER_ID:\n",
        "    raise ValueError(\"USER_ID is not set\")\n",
        "\n",
        "resp = requests.get(f\"{API_BASE}/recommendations\", params={\"user_id\": USER_ID, \"n\": N}, timeout=30)\n",
        "print(\"Status:\", resp.status_code)\n",
        "print(json.dumps(resp.json(), indent=2)[:2000])  # print first ~2KB\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
